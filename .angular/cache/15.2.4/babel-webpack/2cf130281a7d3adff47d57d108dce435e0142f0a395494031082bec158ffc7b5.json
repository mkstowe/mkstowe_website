{"ast":null,"code":"import * as i0 from \"@angular/core\";\nimport * as i1 from \"@angular/platform-browser\";\nexport let VariegataPost3Component = /*#__PURE__*/(() => {\n  class VariegataPost3Component {\n    constructor(titleService) {\n      this.titleService = titleService;\n      this.titleService.setTitle(\"Michael Stowe | Project Variegata\");\n    }\n    ngOnInit() {}\n  }\n  VariegataPost3Component.ɵfac = function VariegataPost3Component_Factory(t) {\n    return new (t || VariegataPost3Component)(i0.ɵɵdirectiveInject(i1.Title));\n  };\n  VariegataPost3Component.ɵcmp = /*@__PURE__*/i0.ɵɵdefineComponent({\n    type: VariegataPost3Component,\n    selectors: [[\"app-variegata-post3\"]],\n    decls: 110,\n    vars: 0,\n    consts: [[1, \"content\"], [1, \"is-bold\"], [1, \"seg\"], [1, \"blog_topic\"], [\"href\", \"https://monkeylearn.com/keyword-extraction/\", \"target\", \"_blank\"], [\"href\", \"https://radimrehurek.com/gensim/\", \"target\", \"_blank\"], [\"href\", \"../../../assets/static-pages/variegata-example1.html\", \"target\", \"_blank\"]],\n    template: function VariegataPost3Component_Template(rf, ctx) {\n      if (rf & 1) {\n        i0.ɵɵelementStart(0, \"div\", 0)(1, \"h2\", 1);\n        i0.ɵɵtext(2, \"February 17, 2021\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(3, \"div\", 2)(4, \"h3\", 3)(5, \"strong\");\n        i0.ɵɵtext(6, \"Progress So Far\");\n        i0.ɵɵelementEnd()();\n        i0.ɵɵelementStart(7, \"p\");\n        i0.ɵɵtext(8, \"This week I explored various algorithms for extracting keywords from a text. Additionally, I continued to build the Flask app to integrate this feature and prepare for next week's additions.\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(9, \"p\");\n        i0.ɵɵtext(10, \"The main technologies I have used so far include:\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(11, \"ul\")(12, \"li\");\n        i0.ɵɵtext(13, \"Python/Flask\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(14, \"li\");\n        i0.ɵɵtext(15, \"Gensim\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(16, \"li\");\n        i0.ɵɵtext(17, \"NLTK\");\n        i0.ɵɵelementEnd()()();\n        i0.ɵɵelementStart(18, \"div\", 2)(19, \"h3\", 3)(20, \"strong\");\n        i0.ɵɵtext(21, \"Keyword Extraction\");\n        i0.ɵɵelementEnd()();\n        i0.ɵɵelementStart(22, \"p\");\n        i0.ɵɵtext(23, \"In order to be able to classify the stories we've been pulling from the web, I needed a good way to extract certain keywords from the text. Doing this will allow the model to be able to create a dictionary to look up which stored documents are associated with a given phrase or genre. For example, if the current event that is being delivered has to do with a mob boss performing slam poetry, our model can use this dictionary to search for events that have associated keywords such as \\\"mafia\\\", \\\"poem\\\", \\\"crime\\\", etc.\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(24, \"p\");\n        i0.ɵɵtext(25, \"Digging deeper into how this actually works, there are a number of algorithms that are often used for keyword extraction. The ones I experimented with are Word Frequency, TF-IDF, RAKE, and TextRank. Before I used any of them, however, I needed to do some text preprocessing. This is a technique for making all of the text consistent. We don't want the algorithm to treat the same word differently when it has different capitalization or accents, for example.\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(26, \"p\");\n        i0.ɵɵtext(27, \"The steps I took to preprocess the text are:\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(28, \"ul\")(29, \"li\");\n        i0.ɵɵtext(30, \"Make all characters lowercase.\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(31, \"li\");\n        i0.ɵɵtext(32, \"Strip any unnecessary whitespace.\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(33, \"li\");\n        i0.ɵɵtext(34, \"Expand contractions (Ex. \\\"won't\\\" becomes \\\"will not\\\").\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(35, \"li\");\n        i0.ɵɵtext(36, \"Remove punctuation and numerical digits.\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(37, \"li\");\n        i0.ɵɵtext(38, \"Remove accents (Ex. \\\"caf\\u00E9\\\" becomes \\\"cafe\\\".\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(39, \"li\");\n        i0.ɵɵtext(40, \"Remove stopwords that just act as filler such as \\\"this\\\" and \\\"because\\\".\");\n        i0.ɵɵelementEnd()();\n        i0.ɵɵelementStart(41, \"p\");\n        i0.ɵɵtext(42, \"Now on to the actual algorithms.\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(43, \"h4\")(44, \"strong\");\n        i0.ɵɵtext(45, \"Word Frequency\");\n        i0.ɵɵelementEnd()();\n        i0.ɵɵelementStart(46, \"p\");\n        i0.ɵɵtext(47, \"Word frequency is the most simple of the algorithms. It simply counts how many times each word appears in a text, and returns the ones that occur most. Easy peasy.\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(48, \"h4\")(49, \"strong\");\n        i0.ɵɵtext(50, \"TF-IDF\");\n        i0.ɵɵelementEnd()();\n        i0.ɵɵelementStart(51, \"p\");\n        i0.ɵɵtext(52, \"Term Frequency - Inverse Document Frequency, or TF-IDF, takes the idea of Word Frequency and makes it a bit more sophisticated. Rather than just looking at one document, it takes all documents available into account. It counts the number of occurrences a word has in its present document, as well as how many documents in which that word appears in total. Using these two values, it can then determine how relevant a word is based on the query.\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(53, \"h4\")(54, \"strong\");\n        i0.ɵɵtext(55, \"RAKE\");\n        i0.ɵɵelementEnd()();\n        i0.ɵɵelementStart(56, \"p\");\n        i0.ɵɵtext(57, \"Rapid Automatic Keyword Extraction, or RAKE is less about finding individual keywords, but rather groups of words to form key phrases. It splits a sentence into individual words to create a matrix that shows how many times each word occurs next to each other word. Using some statistical analysis, it figures out which words fit in best together. Using the example that \");\n        i0.ɵɵelementStart(58, \"a\", 4);\n        i0.ɵɵtext(59, \"MonkeyLearn\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵtext(60, \" gives, the (unprocessed) sentence \\\"Keyword extraction is not that difficult after all. There are many libraries that can help you with keyword extraction. Rapid automatic keyword extraction is one of those.\\\" would yield the phrases \\\"rapid automatic keyword extraction\\\", \\\"keyword extraction\\\", and \\\"many libraries\\\".\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(61, \"h4\")(62, \"strong\");\n        i0.ɵɵtext(63, \"TextRank\");\n        i0.ɵɵelementEnd()();\n        i0.ɵɵelementStart(64, \"p\");\n        i0.ɵɵtext(65, \"TextRank is quite a bit more advanced than RAKE. It creates a directed graph using each word in the text as a vertex. Similarly to RAKE, it then connects each word to each other word based on whether they appear next to each other in the text, in this case creating edges between the verticies. It then determines the overall weight of each vertex based on the part of speech of the word, and how many incoming vs. outgoing edges it has. Finally, it puts these values into a formula to give each word a score, resulting in a list of words or phrases.\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(66, \"p\");\n        i0.ɵɵtext(67, \"Fun Fact: TextRank is inspired by PageRank, which is a similar algorithm used by Google to determine which pages to show based on a search query.\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelement(68, \"br\");\n        i0.ɵɵelementStart(69, \"p\");\n        i0.ɵɵtext(70, \"While all of the listed algorithms gave promising results, I ended up using the TextRank algorithm (using the implementation from the \");\n        i0.ɵɵelementStart(71, \"a\", 5);\n        i0.ɵɵtext(72, \"Gensim\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵtext(73, \" library). However, once I compile more documents and continue to integrate this functionality to the overall project, I suspect that TF-IDF may end up working better as I can essentially create a mini search engine for the events.\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(74, \"p\");\n        i0.ɵɵtext(75, \"To show a larger example of TextRank, these are some top keywords extracted from the \");\n        i0.ɵɵelementStart(76, \"a\", 6);\n        i0.ɵɵtext(77, \"example story\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵtext(78, \" from last week:\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(79, \"ul\")(80, \"li\");\n        i0.ɵɵtext(81, \"behemoth\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(82, \"li\");\n        i0.ɵɵtext(83, \"fight\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(84, \"li\");\n        i0.ɵɵtext(85, \"health\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(86, \"li\");\n        i0.ɵɵtext(87, \"mana\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(88, \"li\");\n        i0.ɵɵtext(89, \"casts\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(90, \"li\");\n        i0.ɵɵtext(91, \"attack\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(92, \"li\");\n        i0.ɵɵtext(93, \"stealing\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(94, \"li\");\n        i0.ɵɵtext(95, \"thunder\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(96, \"li\");\n        i0.ɵɵtext(97, \"wizard\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(98, \"li\");\n        i0.ɵɵtext(99, \"heal\");\n        i0.ɵɵelementEnd()();\n        i0.ɵɵelementStart(100, \"p\");\n        i0.ɵɵtext(101, \"Without even reading the story, it's easy to see from the keywords that it takes place in a more fantastical setting and involves a lot of combat.\");\n        i0.ɵɵelementEnd()();\n        i0.ɵɵelementStart(102, \"div\", 2)(103, \"h3\", 3)(104, \"strong\");\n        i0.ɵɵtext(105, \"Going Forward\");\n        i0.ɵɵelementEnd()();\n        i0.ɵɵelementStart(106, \"p\");\n        i0.ɵɵtext(107, \"The next step is to actually create the dictionary of words and documents so that the model can query them. This may be done with something such as SQL or a Pandas dataframe. The most efficient method will be dependent on how I decide to structure all of the data.\");\n        i0.ɵɵelementEnd();\n        i0.ɵɵelementStart(108, \"p\");\n        i0.ɵɵtext(109, \"From there, it will likely be time to start building a system of building the stories' plot trees and gathering events in realtime.\");\n        i0.ɵɵelementEnd()()();\n      }\n    }\n  });\n  return VariegataPost3Component;\n})();","map":null,"metadata":{},"sourceType":"module","externalDependencies":[]}